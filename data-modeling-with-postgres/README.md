# Sparkify Song Usage Analytics:
A startup called Sparkify wants to understand their userbase better by analyzing the usage of songs they have been listening to on their new music streaming app. The goal of this project is to apply Data Modeling concepts with Postgres and to build an ETL pipeline with fact and dimension tables for Sparkify's analytics team to perform exploratory/data discovery analysis.

# Data Sources:

## Song Dataset
The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. 

## Log Dataset
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations. The log files in this dataset are partitioned by year and month. 

# Schema Design:
After analyzing the data from the two sources, the following entities were required to complete song play analysis:

1. songs - songs in music database
2. users - users in the app
3. artists - artists in music database
4. time - timestamps of records in songplays broken down into specific units
5. songplays -  records in log data associated with song plays i.e. records with page NextSong

- **songs** and **artists** dimension tables were created from the song dataset.
    - songs: song_id, title, artist_id, year, duration
    - artists : artist_id, name, location, latitude, longitude
- **users** and **time** dimension tables were created from the log dataset.
    - users : user_id, first_name, last_name, gender, level
    - time : start_time, hour, day, week, month, year, weekday
- **songplays** fact table was created from both **song** and **log** datasets.
    - songplays : songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

# ETL Pipeline - 

## sql_queries.py:

1. Has SQL queries to drop, create and insert data into fact and dimension tables
2. Two query lists were create to use while running create_tables.py

## create_tables.py:

1. Drops connection (if exists) and creates, connects, gets cursor to to sparkifydb 
2. Runs drop_tables_query first to drop tables if exists
3. Runs_create_tables_query to create tables needed for the ETL pipeline
4. Closes the connection

## etl.py and etl.ipynb (python notebook to analyze and test data before processing)

1. Import libraries that will help establish the connection to sparkifydb, that will run sql scripts from sql_queries.py and import cursors to execute sql queries
2. Create process_song_file function with cursor and filepath as arguments to process .json files in song_data
    1. Create variables using pandas to select columns needed for song dimension table 
        1.1. Use cursor to execute query from sql_queries and insert records from recently created variable
    2. Create variables using pandas to select columns needed for artist dimension table and excute them to insert records
        2.1. Use cursor to execute query from sql_queries and insert records from recently created variable
3. Create process_log_file function with cursor and filepath as arguments to process .json files in log_data
    1. Create variables using pandas to select columns needed for time dimension table 
        - Use cursor to execute query from sql_queries and insert records from recently created variable
    2. Create variables using pandas to select columns needed for user dimension table and excute them to insert records
        - Use cursor to execute query from sql_queries and insert records from recently created variable
    3. Create variables using pandas to select columns needed for songplay fact table and excute them to insert records
        - Use cursor to execute query from sql_queries and insert records from recently created variable
4. Create process_data function to:
    1. Get all files matching extension from directory
    2. Get total number of files found
    3. Iterate over files and process
5. Create driver function to:
    1. Connect to DB and cursor
    2. Processes data into process_song_file and process_log_file
    3. Closes the connection

## test.ipynb:

Jupiter notebook with sample queries to test connection, creation of tables and insertion of records.

# Process flow:

1. python create_tables.py
    - update sql_queries as per instructions in etl.ipynb
    - run create_tables.py after every update
    - run test.ipynb to test successful completion of update
    - restart kernel after every update
    - finally, create_tables.py should run without errors
2. python etl.py
    - finally etl.py should run without errors
    
# Sample queries for analysis:

## top 10 users with most songplays:

select top 10 concat(u.first_name,' ',u.last_name) as user_name,  
count(distinct  s.songplay_id) as distinct_songplays  
from songplays s  
left join users u on s.user_id = u.user_id  
group by 1  
order by 2 desc

## top 10 most played songs:

select top 10 so.title as song_name,  
count(distinct  s.songplay_id) as distinct_songplays  
from songplays so  
left join users u on s.song_id = so.song_id  
group by 1  
order by 2 desc

## top 10 most played artists:

select top 10 a.name as artist_name,  
count(distinct  s.songplay_id) as distinct_songplays  
from songplays so  
left join artists a on s.artist_id = a.artist_id  
group by 1  
order by 2 desc